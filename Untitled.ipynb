{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QLearning_MountainCar_sol import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: At episode: 1 - Reward mean from last 100 episodes: -200.0. - LR:1.0000 - eps:0.5000\n",
      "0: At episode: 1001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 2001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 3001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 4001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 5001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 6001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 7001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 8001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: At episode: 9001 - Reward mean from last 100 episodes: -200.0. - LR:0.0010 - eps:0.0010\n",
      "0: Average score of solution on a dry run=  -1.25\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'display_frames_as_gif' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3080a75cd53f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_car\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/rl/Lab - 5/Lab - 5/Lab5/QLearning_MountainCar_sol.py\u001b[0m in \u001b[0;36mtrain_car\u001b[0;34m(initial_lr, min_lr, lr_decay, gamma, epsilon_start, epsilon_decay, epsilon_end)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m grid = {\n",
      "\u001b[0;32m~/Documents/rl/Lab - 5/Lab - 5/Lab5/QLearning_MountainCar_sol.py\u001b[0m in \u001b[0;36mtrain_q_learning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m#print(solution_policy_scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution_policy_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m#return solution_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rl/Lab - 5/Lab - 5/Lab5/QLearning_MountainCar_sol.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, policy, render)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdisplay_frames_as_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_frames_as_gif' is not defined"
     ]
    }
   ],
   "source": [
    "train_car(1.0, 0.001, 0.5, 0.2, 0.5, 0.5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40055728,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2 , -0.07], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - AVG100 0.00\n",
      "Episode 1000 - AVG100 -200.00\n",
      "Episode 2000 - AVG100 -200.00\n",
      "Episode 3000 - AVG100 -200.00\n",
      "Episode 4000 - AVG100 -200.00\n",
      "Episode 5000 - AVG100 -200.00\n",
      "Episode 6000 - AVG100 -200.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-aa24aefb3d94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mrandom_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-aa24aefb3d94>\u001b[0m in \u001b[0;36mtrain_q\u001b[0;34m(self, episodes, policy)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"Episode {episode_i} - AVG100 {stats['moving_avg_100']:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-aa24aefb3d94>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, episode_i, policy)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-aa24aefb3d94>\u001b[0m in \u001b[0;36mrandom_policy\u001b[0;34m(self, position, speed, epsilon)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, \n",
    "                 episode_length = 200,\n",
    "                 \n",
    "                 position_step = 0.1, \n",
    "                 speed_step = 0.01,\n",
    "                 \n",
    "                 gamma = 0.8,\n",
    "                 \n",
    "                 lr_initial = 1.0, \n",
    "                 lr_decay   = 0.999,\n",
    "                 lr_min     = 0.01,\n",
    "                 \n",
    "                 epsilon_initial = 1.0, \n",
    "                 epsilon_decay   = 0.999, \n",
    "                 epsilon_min     = 0.01):\n",
    "        \n",
    "        self.episode_length = episode_length\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.lr_initial = lr_initial\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_min = lr_min\n",
    "        \n",
    "        self.epsilon_initial = epsilon_initial\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.env = env\n",
    "        self.steps = np.array([position_step, speed_step])\n",
    "        \n",
    "        self.n_action = self.env.action_space.n\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        return np.vectorize(np.round)((state - self.env.low) / self.steps).astype(int)\n",
    "\n",
    "    def random_policy(self, position, speed, epsilon=0):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def e_greedy_policy(self, position, speed, epsilon=0):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return random_policy()\n",
    "        return np.argmax(self.q[position][speed])\n",
    "    \n",
    "    def get_epsilon(self, i):\n",
    "        return max(self.epsilon_initial * (self.epsilon_decay ** i), self.epsilon_min)\n",
    "    \n",
    "    def get_lr(self, i):\n",
    "        return max(self.lr_initial * (self.lr_decay ** i), self.lr_min)\n",
    "        \n",
    "    def run_episode(self, episode_i, policy):\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(self.episode_length):\n",
    "            position, speed = self.discretize_state(state)\n",
    "            action = policy(position, speed, self.get_epsilon(episode_i))\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                   \n",
    "        return total_reward\n",
    "    \n",
    "    def train_q(self, episodes, policy):\n",
    "        q = defaultdict(lambda: defaultdict(\n",
    "            lambda: np.random.uniform(-1, 1, self.n_action)\n",
    "        ))\n",
    "        \n",
    "        stats = {\n",
    "            \"moving_total_100\": 0,\n",
    "            \"moving_avg_100\": 0,\n",
    "            \"last_100_rewards\": deque(),\n",
    "        }\n",
    "        \n",
    "        for episode_i in range(episodes):\n",
    "            if episode_i % 1000 == 0:\n",
    "                print (f\"Episode {episode_i} - AVG100 {stats['moving_avg_100']:.2f}\")\n",
    "            \n",
    "            reward = self.run_episode(episode_i, policy)\n",
    "            \n",
    "            if reward > -self.episode_length:\n",
    "                print(f\"On episode {episode_i} we got a better score: {reward}\")\n",
    "            \n",
    "            stats[\"last_100_rewards\"].append(reward)\n",
    "            stats[\"moving_total_100\"] += reward\n",
    "            while len(stats[\"last_100_rewards\"]) > 100:\n",
    "                oldest_reward = stats[\"last_100_rewards\"].popleft()\n",
    "                stats[\"moving_total_100\"] -= oldest_reward\n",
    "        \n",
    "            stats[\"moving_avg_100\"] = stats[\"moving_total_100\"] / len(stats[\"last_100_rewards\"])\n",
    "        \n",
    "        return q\n",
    "    \n",
    "a = Agent(env)\n",
    "random_q = a.train_q(50000, a.random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40262126, 0.05732873, 0.43054252])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(-1, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-199.99"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-200*99+(-199))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4182659  0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 -9999999999999999\n",
      "episode 100 -200.0\n",
      "episode 200 -200.0\n",
      "episode 300 -200.0\n",
      "episode 400 -200.0\n",
      "episode 500 -200.0\n",
      "episode 600 -200.0\n",
      "episode 700 -200.0\n",
      "episode 800 -200.0\n",
      "episode 900 -200.0\n",
      "episode 1000 -200.0\n",
      "episode 1100 -200.0\n",
      "episode 1200 -200.0\n",
      "episode 1300 -200.0\n",
      "episode 1400 -200.0\n",
      "episode 1500 -200.0\n",
      "episode 1600 -200.0\n",
      "episode 1700 -200.0\n",
      "episode 1800 -200.0\n",
      "episode 1900 -200.0\n",
      "episode 2000 -200.0\n",
      "episode 2100 -200.0\n",
      "episode 2200 -200.0\n",
      "episode 2300 -200.0\n",
      "episode 2400 -200.0\n",
      "episode 2500 -200.0\n",
      "episode 2600 -200.0\n",
      "episode 2700 -200.0\n",
      "episode 2800 -200.0\n",
      "episode 2900 -200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "best = -9999999999999999\n",
    "for i in range(3000):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print (f\"episode {i} {best}\")\n",
    "        \n",
    "    total_reward = 0\n",
    "    for t in range(201):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward+=reward\n",
    "        \n",
    "        if observation[0] >= 0.5:\n",
    "            print (f\"Solved in episode {i}!!!!\")\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    best = max(best, total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(10):\\n    observation = env.reset()\\n    for t in range(100):\\n        env.render()\\n        print(observation)\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       "  \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(10):\\n    observation = env.reset()\\n    for t in range(2000):\\n        env.render()\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       "  \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(100):\\n    observation = env.reset()\\n    print (i)\\n    for t in range(200):\\n        env.render()\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       "  'env.close()',\n",
       "  'import gc',\n",
       "  'import gc\\ndir(gc)',\n",
       "  'import gc\\ngc.collect()',\n",
       "  'import gc\\ngc.collect()\\n\\n__globals__',\n",
       "  'import gc\\nglobals()',\n",
       "  'import gc\\ndel _i2',\n",
       "  'import gc\\nglobals()'],\n",
       " '_oh': {6: ['DEBUG_COLLECTABLE',\n",
       "   'DEBUG_LEAK',\n",
       "   'DEBUG_SAVEALL',\n",
       "   'DEBUG_STATS',\n",
       "   'DEBUG_UNCOLLECTABLE',\n",
       "   '__doc__',\n",
       "   '__loader__',\n",
       "   '__name__',\n",
       "   '__package__',\n",
       "   '__spec__',\n",
       "   'callbacks',\n",
       "   'collect',\n",
       "   'disable',\n",
       "   'enable',\n",
       "   'freeze',\n",
       "   'garbage',\n",
       "   'get_count',\n",
       "   'get_debug',\n",
       "   'get_freeze_count',\n",
       "   'get_objects',\n",
       "   'get_referents',\n",
       "   'get_referrers',\n",
       "   'get_stats',\n",
       "   'get_threshold',\n",
       "   'is_tracked',\n",
       "   'isenabled',\n",
       "   'set_debug',\n",
       "   'set_threshold',\n",
       "   'unfreeze'],\n",
       "  7: 453,\n",
       "  9: {...}},\n",
       " '_dh': ['/home/littlewho/Documents/rl/Lab - 5/Lab - 5/Lab5'],\n",
       " 'In': ['',\n",
       "  \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(10):\\n    observation = env.reset()\\n    for t in range(100):\\n        env.render()\\n        print(observation)\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       "  \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(10):\\n    observation = env.reset()\\n    for t in range(2000):\\n        env.render()\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       "  \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(100):\\n    observation = env.reset()\\n    print (i)\\n    for t in range(200):\\n        env.render()\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       "  'env.close()',\n",
       "  'import gc',\n",
       "  'import gc\\ndir(gc)',\n",
       "  'import gc\\ngc.collect()',\n",
       "  'import gc\\ngc.collect()\\n\\n__globals__',\n",
       "  'import gc\\nglobals()',\n",
       "  'import gc\\ndel _i2',\n",
       "  'import gc\\nglobals()'],\n",
       " 'Out': {6: ['DEBUG_COLLECTABLE',\n",
       "   'DEBUG_LEAK',\n",
       "   'DEBUG_SAVEALL',\n",
       "   'DEBUG_STATS',\n",
       "   'DEBUG_UNCOLLECTABLE',\n",
       "   '__doc__',\n",
       "   '__loader__',\n",
       "   '__name__',\n",
       "   '__package__',\n",
       "   '__spec__',\n",
       "   'callbacks',\n",
       "   'collect',\n",
       "   'disable',\n",
       "   'enable',\n",
       "   'freeze',\n",
       "   'garbage',\n",
       "   'get_count',\n",
       "   'get_debug',\n",
       "   'get_freeze_count',\n",
       "   'get_objects',\n",
       "   'get_referents',\n",
       "   'get_referrers',\n",
       "   'get_stats',\n",
       "   'get_threshold',\n",
       "   'is_tracked',\n",
       "   'isenabled',\n",
       "   'set_debug',\n",
       "   'set_threshold',\n",
       "   'unfreeze'],\n",
       "  7: 453,\n",
       "  9: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f169358a5e0>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f16904ec7f0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f16904ec7f0>,\n",
       " '_': {...},\n",
       " '__': 453,\n",
       " '___': ['DEBUG_COLLECTABLE',\n",
       "  'DEBUG_LEAK',\n",
       "  'DEBUG_SAVEALL',\n",
       "  'DEBUG_STATS',\n",
       "  'DEBUG_UNCOLLECTABLE',\n",
       "  '__doc__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__spec__',\n",
       "  'callbacks',\n",
       "  'collect',\n",
       "  'disable',\n",
       "  'enable',\n",
       "  'freeze',\n",
       "  'garbage',\n",
       "  'get_count',\n",
       "  'get_debug',\n",
       "  'get_freeze_count',\n",
       "  'get_objects',\n",
       "  'get_referents',\n",
       "  'get_referrers',\n",
       "  'get_stats',\n",
       "  'get_threshold',\n",
       "  'is_tracked',\n",
       "  'isenabled',\n",
       "  'set_debug',\n",
       "  'set_threshold',\n",
       "  'unfreeze'],\n",
       " '_i': 'import gc\\ndel _i2',\n",
       " '_ii': 'import gc\\nglobals()',\n",
       " '_iii': 'import gc\\ngc.collect()\\n\\n__globals__',\n",
       " '_i1': \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(10):\\n    observation = env.reset()\\n    for t in range(100):\\n        env.render()\\n        print(observation)\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       " 'gym': <module 'gym' from '/home/littlewho/anaconda3/envs/data_sci/lib/python3.8/site-packages/gym/__init__.py'>,\n",
       " 'env': <TimeLimit<MountainCarEnv<MountainCar-v0>>>,\n",
       " 'i': 18,\n",
       " 'observation': array([-0.572803  , -0.01033423], dtype=float32),\n",
       " 't': 92,\n",
       " 'action': 2,\n",
       " 'reward': -1.0,\n",
       " 'done': False,\n",
       " 'info': {},\n",
       " '_i3': \"import gym\\n\\nenv = gym.make('MountainCar-v0')\\nfor i in range(100):\\n    observation = env.reset()\\n    print (i)\\n    for t in range(200):\\n        env.render()\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        if done:\\n            break\\nenv.close()\",\n",
       " '_i4': 'env.close()',\n",
       " '_i5': 'import gc',\n",
       " 'gc': <module 'gc' (built-in)>,\n",
       " '_i6': 'import gc\\ndir(gc)',\n",
       " '_6': ['DEBUG_COLLECTABLE',\n",
       "  'DEBUG_LEAK',\n",
       "  'DEBUG_SAVEALL',\n",
       "  'DEBUG_STATS',\n",
       "  'DEBUG_UNCOLLECTABLE',\n",
       "  '__doc__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__spec__',\n",
       "  'callbacks',\n",
       "  'collect',\n",
       "  'disable',\n",
       "  'enable',\n",
       "  'freeze',\n",
       "  'garbage',\n",
       "  'get_count',\n",
       "  'get_debug',\n",
       "  'get_freeze_count',\n",
       "  'get_objects',\n",
       "  'get_referents',\n",
       "  'get_referrers',\n",
       "  'get_stats',\n",
       "  'get_threshold',\n",
       "  'is_tracked',\n",
       "  'isenabled',\n",
       "  'set_debug',\n",
       "  'set_threshold',\n",
       "  'unfreeze'],\n",
       " '_i7': 'import gc\\ngc.collect()',\n",
       " '_7': 453,\n",
       " '_i8': 'import gc\\ngc.collect()\\n\\n__globals__',\n",
       " '_i9': 'import gc\\nglobals()',\n",
       " '_9': {...},\n",
       " '_i10': 'import gc\\ndel _i2',\n",
       " '_i11': 'import gc\\nglobals()'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "globals()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
